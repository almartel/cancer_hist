{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train U-Net models\n",
    "* Some issues with using keras for data augmentation, the standard functions may only support 3 channel images?\n",
    "\n",
    "### Manual Data Augmentation\n",
    "* First try using a simple augmentation strategy, only using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy.ndimage import rotate\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import metrics\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Reshape, Input, concatenate, Conv2DTranspose\n",
    "from keras.layers.core import Activation, Dense, Lambda\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "############ DATA GENERATORS\n",
    "def data_gen_aug_combined(file_loc, mask_loc, batch_size, square_rot_p=.3, seed=101):\n",
    "    # square_rot_p is the prob of using a 90x rotation, otherwise sample from 360. Possibly not useful\n",
    "    # translate is maximum number of pixels to translate by\n",
    "    # crops are done \n",
    "    square_rot_p = int(square_rot_p)\n",
    "    np.random.seed(seed)\n",
    "    all_files=glob.glob(os.path.join(file_loc, '*'))\n",
    "    all_masks=[]\n",
    "\n",
    "    all_files = [loc for loc in all_files if loc.rsplit('.', 1)[-1] in ['tif']]\n",
    "\n",
    "#     for file in all_files:\n",
    "#         im_name = str(file.rsplit('.', 1)[-2].rsplit('/', 1)[1].rsplit('_', 1)[0].replace(\" \", \"_\"))\n",
    "#         loc = os.path.join(mask_loc, im_name+'.npy')\n",
    "#         all_masks.append(loc)\n",
    "        \n",
    "    for file in all_files:\n",
    "        im_name = str(file.rsplit('.', 1)[-2].rsplit('/', 1)[1])\n",
    "        loc = os.path.join(mask_loc, im_name+'.tif')\n",
    "        all_masks.append(loc)\n",
    "\n",
    "    while 1:\n",
    "        c = list(zip(all_files, all_masks))\n",
    "        np.random.shuffle(c)\n",
    "        all_files, all_masks = zip(*c)\n",
    "\n",
    "        num_batches = int(np.floor(len(all_files)/batch_size))-1\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            x=[]\n",
    "            y=[]\n",
    "            batch_files = all_files[batch_size*batch:batch_size*(batch+1)]\n",
    "            batch_files_mask = all_masks[batch_size*batch:batch_size*(batch+1)]\n",
    "\n",
    "            for index in range(len(batch_files)):\n",
    "                image_loc = batch_files[index]\n",
    "                mask_loc = batch_files_mask[index]\n",
    "\n",
    "                # load the image\n",
    "                image = Image.open(image_loc)\n",
    "                width, height = image.size\n",
    "                image = np.reshape(np.array(image.getdata()), (height, width, 3))\n",
    "\n",
    "                #load the mask\n",
    "                mask = Image.open(mask_loc)\n",
    "                width, height = mask.size\n",
    "                mask = np.reshape(np.array(mask.getdata()), (height, width, 4))\n",
    "                \n",
    "                # All the randomness:\n",
    "                height, width = np.shape(image)[0], np.shape(image)[1]\n",
    "                crop_row = np.random.randint(0, height-320)\n",
    "                crop_col = np.random.randint(0, width-368)\n",
    "                flip_vert = np.random.randint(0, 2)\n",
    "                flip_hor = np.random.randint(0, 2)\n",
    "\n",
    "                # APPLY AUGMENTATION:\n",
    "                # flips\n",
    "                if flip_vert:\n",
    "                    image = np.flipud(image)\n",
    "                    mask = np.flipud(mask)\n",
    "\n",
    "                if flip_hor:\n",
    "                    image = np.fliplr(image)\n",
    "                    mask = np.fliplr(mask)\n",
    "\n",
    "                # rotation\n",
    "                square_rot =  bool((np.random.uniform(0, 1, 1)<square_rot_p))\n",
    "                if square_rot:  # maybe this is dumb, but it cant hurt\n",
    "                    rotations=['0', '90', '180', '270']\n",
    "                    angle = int(random.choice(rotations))\n",
    "                    image = rotate(image, angle, reshape=False)\n",
    "                    mask = rotate(mask, angle, reshape=False)\n",
    "\n",
    "                else:\n",
    "                    angle = np.random.uniform(0, 360, 1)\n",
    "                    image = rotate(image, angle, reshape=False)\n",
    "                    mask = rotate(mask, angle, reshape=False)\n",
    " \n",
    "                # crop to 320 x 360 so it will fit into network, and for data augmentation\n",
    "                image = image[crop_row:crop_row+320, crop_col:crop_col+368]\n",
    "                mask = mask[crop_row:crop_row+320, crop_col:crop_col+368]\n",
    "\n",
    "                image = image/255.0 # make pixels in [0,1] \n",
    "                x.append(image)\n",
    "                y.append(mask)\n",
    "            x=np.array(x)\n",
    "            y=np.array(y)\n",
    "            yield (x, y)\n",
    "\n",
    "\n",
    "def data_gen_combined(file_loc, mask_loc, batch_size, seed=101):\n",
    "    np.random.seed(seed)\n",
    "    all_files=glob.glob(os.path.join(file_loc, '*'))\n",
    "    all_files = [loc for loc in all_files if loc.rsplit('.', 1)[-1] in ['tif']]\n",
    "    all_masks=[]\n",
    "    for file in all_files:\n",
    "        im_name = str(file.rsplit('.', 1)[-2].rsplit('/', 1)[1])\n",
    "        loc = os.path.join(mask_loc, im_name+'.tif')\n",
    "        all_masks.append(loc)\n",
    "\n",
    "    all_files = [loc for loc in all_files if loc.rsplit('.', 1)[-1] in ['tif']]\n",
    "\n",
    "    while 1:\n",
    "        c = list(zip(all_files, all_masks))\n",
    "        np.random.shuffle(c)\n",
    "        all_files, all_masks = zip(*c)\n",
    "        \n",
    "        num_batches = int(np.floor(len(all_files)/batch_size))-1\n",
    "        for batch in range(num_batches):\n",
    "            x=[]\n",
    "            y=[]\n",
    "            batch_files = all_files[batch_size*batch:batch_size*(batch+1)]\n",
    "            batch_files_mask = all_masks[batch_size*batch:batch_size*(batch+1)]\n",
    "\n",
    "            for index in range(len(batch_files)):\n",
    "                image_loc = batch_files[index]\n",
    "                mask_loc = batch_files_mask[index]\n",
    "\n",
    "                # load the image\n",
    "                image = Image.open(image_loc)\n",
    "                width, height = image.size\n",
    "                image = np.reshape(np.array(image.getdata()), (height, width, 3))\n",
    "\n",
    "                #load the mask\n",
    "                mask = Image.open(mask_loc)\n",
    "                width, height = mask.size\n",
    "                mask = np.reshape(np.array(mask.getdata()), (height, width, 4))\n",
    "                \n",
    "                ################################ IMPLEMENT::::\n",
    "                # We will pad the imput to make them all the same size:\n",
    "                \n",
    "                # make it the same size as the training examples\n",
    "                height, width = np.shape(image)[0], np.shape(image)[1]\n",
    "                crop_row = np.random.randint(0, height-320)\n",
    "                crop_col = np.random.randint(0, width-368)\n",
    "\n",
    "                # crop to 320 x 360 so it will fit into network, and for data augmentation\n",
    "                image = image[crop_row:crop_row+320, crop_col:crop_col+368]\n",
    "                mask = mask[crop_row:crop_row+320, crop_col:crop_col+368]\n",
    "\n",
    "                image = image/255.0 # make pixels in [0,1]     \n",
    "                x.append(image)\n",
    "                y.append(mask)\n",
    "\n",
    "            x=np.array(x)\n",
    "            y=np.array(y)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "#### Loss\n",
    "Loss Function is different than the usual dice coefficient. We won't measure overlap. It is made of two parts:\n",
    "1. MSE on the distance to the nearest nuclei.\n",
    "2. Class of the nearest nuclei\n",
    "Both of these parts should be 0 if the nearest nuclei is over 20 pixels away? At least I think so. For sure the distance is meaningless, and the classifation would just add some noise to the model.\n",
    "\n",
    "#### Model\n",
    "* Test a models smaller and larger than the original U-Net.\n",
    "* Try adding batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distance loss function\n",
    "def distance_loss(y_true, y_pred):\n",
    "    weight = .5 # how mush does the distance matter compared to the cross entropy (fast ai used .001 for 4 more uncertain ones)\n",
    "    # Already scaled distance values between (0,1). Cut off ones larger because this doesn't hurt the prediction\n",
    "#     K.int_shape(y_true)\n",
    "#     K.int_shape(y_pred)\n",
    "#     y_pred_clip = K.clip(y_pred[:, :, 0], -1, 1)\n",
    "#     K.int_shape(y_pred_clip)\n",
    "    distance_loss = K.binary_crossentropy(y_pred[:, :, :, 0], y_true[:, :, :, 0])\n",
    "#     K.int_shape(distance_loss)\n",
    "    \n",
    "    cross_entropy = K.categorical_crossentropy(y_true[:, :, :, 1:], y_pred[:, :, :, 1:])    \n",
    "#     K.int_shape(cross_entropy)\n",
    "\n",
    "    return(distance_loss*weight+(1-weight)*cross_entropy)\n",
    "\n",
    "\n",
    "# Remove all the predictions from the cost that are under 20 away for cross entropy. Not for MSE because it should learn easily\n",
    "# def distance_loss_under20(y_true, y_pred):\n",
    "#     weight = .05 # how mush does the distance matter compared to the cross entropy (fast ai used .001 for 4 more uncertain ones)\n",
    "#     # Clip the distance values to be less than 20 :\n",
    "#     y_pred[:, :, 0] = K.clip(y_pred[:, :, 0], -1, 1)\n",
    "#     mse = K.mean(K.square(y_pred[:, :, 0] - y_true[:, :, 0]), axis=-1)\n",
    "    \n",
    "#     # Only look at the elements with a distance of less than 20  pixels from the nuclei.\n",
    "#     y_true_clip = \n",
    "#     y_pred_clip = \n",
    "#     cross_entropy = categorical_crossentropy(y_true[:, :, 1:], y_pred[:, :, 1:])    \n",
    "#     return(mse*weight+(1-weight)*cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ UNET ARCHITECTURES \n",
    "\n",
    "def unet_standard(learning_rate=.0001):\n",
    "    input_shape = (None, None, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(img_input)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10_dist = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "    conv10_cross_entropy = Conv2D(3, (1, 1), activation='softmax')(conv9)\n",
    "    output = concatenate([conv10_dist, conv10_cross_entropy])\n",
    "\n",
    "    model = Model(img_input, output)\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=distance_loss, metrics=[distance_loss])\n",
    "    return model\n",
    "\n",
    "def unet_mid(learning_rate=.0001):\n",
    "    input_shape = (None, None, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(img_input)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv5 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5), conv3], axis=3)\n",
    "    conv6 = Conv2D(128, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6), conv2], axis=3)\n",
    "    conv7 = Conv2D(64, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv7), conv1], axis=3)\n",
    "    conv8 = Conv2D(32, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    conv9_dist = Conv2D(1, (1, 1), activation='sigmoid')(conv8)\n",
    "    conv9_cross_entropy = Conv2D(3, (1, 1), activation='softmax')(conv8)\n",
    "    output = concatenate([conv9_dist, conv9_cross_entropy])\n",
    "\n",
    "    model = Model(img_input, output)\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=distance_loss, metrics=[distance_loss])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(x,\n",
    "              filters,\n",
    "              num_row,\n",
    "              num_col,\n",
    "              dropout, \n",
    "              padding='same',\n",
    "              strides=(1, 1),\n",
    "              activation='relu'):\n",
    "    x = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    x = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def unet_paper(learning_rate=.0001):\n",
    "    input_shape = (None, None, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    conv1 = conv_block(img_input, 32, 3, 3, dropout = .1, padding='same', strides=(1, 1), activation='relu')\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = conv_block(pool1, 64, 3, 3, dropout = .1, padding='same', strides=(1, 1), activation='relu')\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = conv_block(pool2, 128, 3, 3, dropout = .1, padding='same', strides=(1, 1), activation='relu')\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = conv_block(pool3, 128, 3, 3, dropout = .1, padding='same', strides=(1, 1), activation='relu')\n",
    "\n",
    "    up5 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv4), conv3], axis=3)\n",
    "    conv5 = conv_block(up5, 128, 3, 3, dropout = .1, padding='same', strides=(1, 1), activation='relu')\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5), conv2], axis=3)\n",
    "    conv6 = conv_block(up6, 64, 3, 3, dropout = .1, padding='same', strides=(1, 1), activation='relu')\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv6), conv1], axis=3)\n",
    "    conv7 = conv_block(up7, 32, 3, 3, dropout = .1, padding='same', strides=(1, 1), activation='relu')\n",
    "    \n",
    "    conv8_dist = Conv2D(1, (1, 1), activation='sigmoid')(conv7)\n",
    "    conv8_cross_entropy = Conv2D(3, (1, 1), activation='softmax')(conv7)\n",
    "    output = concatenate([conv8_dist, conv8_cross_entropy])\n",
    "    \n",
    "    model = Model(img_input, output)\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=distance_loss, metrics=[distance_loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rbbidart/project/rbbidart/cancer_hist/full_slides2/valid/0\n",
      "num_train 12\n",
      "num_valid 4\n",
      "validation_steps 1.0\n",
      "Epoch 1/100\n",
      "2/3 [===================>..........] - ETA: 9s - loss: 153.2938 - distance_loss: 153.2938 Epoch 00000: distance_loss improved from inf to 148.62124, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.00-148.62.hdf5\n",
      "3/3 [==============================] - 107s - loss: 148.6212 - distance_loss: 148.6212 - val_loss: 1817.5200 - val_distance_loss: 1817.5199\n",
      "Epoch 2/100\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 137.6425 - distance_loss: 137.6425Epoch 00001: distance_loss improved from 148.62124 to 148.41958, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.01-148.42.hdf5\n",
      "3/3 [==============================] - 389s - loss: 148.4196 - distance_loss: 148.4196 - val_loss: 1979.5701 - val_distance_loss: 1979.5703\n",
      "Epoch 3/100\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 137.6197 - distance_loss: 137.6197Epoch 00002: distance_loss improved from 148.41958 to 132.25230, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.02-132.25.hdf5\n",
      "3/3 [==============================] - 139s - loss: 132.2523 - distance_loss: 132.2523 - val_loss: 1396.4583 - val_distance_loss: 1396.4585\n",
      "Epoch 4/100\n",
      "2/3 [===================>..........] - ETA: 82s - loss: 133.0340 - distance_loss: 133.0340Epoch 00003: distance_loss improved from 132.25230 to 130.94454, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.03-130.94.hdf5\n",
      "3/3 [==============================] - 311s - loss: 130.9445 - distance_loss: 130.9445 - val_loss: 1454.1101 - val_distance_loss: 1454.1099\n",
      "Epoch 5/100\n",
      "2/3 [===================>..........] - ETA: 89s - loss: 127.6025 - distance_loss: 127.6025Epoch 00004: distance_loss improved from 130.94454 to 127.33455, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.04-127.33.hdf5\n",
      "3/3 [==============================] - 419s - loss: 127.3345 - distance_loss: 127.3346 - val_loss: 1506.8855 - val_distance_loss: 1506.8861\n",
      "Epoch 6/100\n",
      "2/3 [===================>..........] - ETA: 70s - loss: 132.9682 - distance_loss: 132.9682Epoch 00005: distance_loss did not improve\n",
      "3/3 [==============================] - 380s - loss: 133.1369 - distance_loss: 133.1369 - val_loss: 1483.7692 - val_distance_loss: 1483.7694\n",
      "Epoch 7/100\n",
      "2/3 [===================>..........] - ETA: 38s - loss: 127.1748 - distance_loss: 127.1748Epoch 00006: distance_loss improved from 127.33455 to 122.89730, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.06-122.90.hdf5\n",
      "3/3 [==============================] - 198s - loss: 122.8973 - distance_loss: 122.8973 - val_loss: 1113.6174 - val_distance_loss: 1113.6178\n",
      "Epoch 8/100\n",
      "2/3 [===================>..........] - ETA: 53s - loss: 122.8441 - distance_loss: 122.8441 Epoch 00007: distance_loss improved from 122.89730 to 120.13072, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.07-120.13.hdf5\n",
      "3/3 [==============================] - 170s - loss: 120.1307 - distance_loss: 120.1307 - val_loss: 299.3292 - val_distance_loss: 299.3292\n",
      "Epoch 9/100\n",
      "2/3 [===================>..........] - ETA: 43s - loss: 127.9845 - distance_loss: 127.9845Epoch 00008: distance_loss did not improve\n",
      "3/3 [==============================] - 240s - loss: 127.7261 - distance_loss: 127.7262 - val_loss: 183.0668 - val_distance_loss: 183.0668\n",
      "Epoch 10/100\n",
      "2/3 [===================>..........] - ETA: 93s - loss: 152.0898 - distance_loss: 152.0898 Epoch 00009: distance_loss did not improve\n",
      "3/3 [==============================] - 362s - loss: 147.9084 - distance_loss: 147.9084 - val_loss: 147.3283 - val_distance_loss: 147.3283\n",
      "Epoch 11/100\n",
      "2/3 [===================>..........] - ETA: 44s - loss: 130.6462 - distance_loss: 130.6462 Epoch 00010: distance_loss did not improve\n",
      "3/3 [==============================] - 228s - loss: 123.6018 - distance_loss: 123.6018 - val_loss: 149.9226 - val_distance_loss: 149.9226\n",
      "Epoch 12/100\n",
      "2/3 [===================>..........] - ETA: 94s - loss: 125.1288 - distance_loss: 125.1288 Epoch 00011: distance_loss did not improve\n",
      "3/3 [==============================] - 489s - loss: 128.5632 - distance_loss: 128.5632 - val_loss: 163.3717 - val_distance_loss: 163.3717\n",
      "Epoch 13/100\n",
      "2/3 [===================>..........] - ETA: 113s - loss: 123.7961 - distance_loss: 123.7961Epoch 00012: distance_loss did not improve\n",
      "3/3 [==============================] - 365s - loss: 129.9687 - distance_loss: 129.9687 - val_loss: 354.3569 - val_distance_loss: 354.3569\n",
      "Epoch 14/100\n",
      "2/3 [===================>..........] - ETA: 77s - loss: 117.5143 - distance_loss: 117.5143 Epoch 00013: distance_loss improved from 120.13072 to 115.04145, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.13-115.04.hdf5\n",
      "3/3 [==============================] - 356s - loss: 115.0414 - distance_loss: 115.0414 - val_loss: 444.0446 - val_distance_loss: 444.0447\n",
      "Epoch 15/100\n",
      "2/3 [===================>..........] - ETA: 25s - loss: 113.8693 - distance_loss: 113.8693Epoch 00014: distance_loss did not improve\n",
      "3/3 [==============================] - 189s - loss: 116.9746 - distance_loss: 116.9746 - val_loss: 377.6012 - val_distance_loss: 377.6011\n",
      "Epoch 16/100\n",
      "2/3 [===================>..........] - ETA: 95s - loss: 112.0095 - distance_loss: 112.0095Epoch 00015: distance_loss improved from 115.04145 to 114.26816, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.15-114.27.hdf5\n",
      "3/3 [==============================] - 404s - loss: 114.2682 - distance_loss: 114.2682 - val_loss: 249.9587 - val_distance_loss: 249.9587\n",
      "Epoch 17/100\n",
      "2/3 [===================>..........] - ETA: 19s - loss: 111.3501 - distance_loss: 111.3501Epoch 00016: distance_loss did not improve\n",
      "3/3 [==============================] - 206s - loss: 115.2296 - distance_loss: 115.2296 - val_loss: 166.9842 - val_distance_loss: 166.9842\n",
      "Epoch 18/100\n",
      "2/3 [===================>..........] - ETA: 84s - loss: 123.2486 - distance_loss: 123.2485 Epoch 00017: distance_loss did not improve\n",
      "3/3 [==============================] - 374s - loss: 122.9214 - distance_loss: 122.9214 - val_loss: 126.2913 - val_distance_loss: 126.2913\n",
      "Epoch 19/100\n",
      "2/3 [===================>..........] - ETA: 65s - loss: 120.1522 - distance_loss: 120.1522Epoch 00018: distance_loss did not improve\n",
      "3/3 [==============================] - 316s - loss: 119.8027 - distance_loss: 119.8027 - val_loss: 123.8618 - val_distance_loss: 123.8617\n",
      "Epoch 20/100\n",
      "2/3 [===================>..........] - ETA: 176s - loss: 126.7870 - distance_loss: 126.7870Epoch 00019: distance_loss did not improve\n",
      "3/3 [==============================] - 617s - loss: 124.9048 - distance_loss: 124.9048 - val_loss: 129.6403 - val_distance_loss: 129.6402\n",
      "Epoch 21/100\n",
      "2/3 [===================>..........] - ETA: 94s - loss: 124.3386 - distance_loss: 124.3386Epoch 00020: distance_loss did not improve\n",
      "3/3 [==============================] - 343s - loss: 119.0077 - distance_loss: 119.0077 - val_loss: 122.7550 - val_distance_loss: 122.7549\n",
      "Epoch 22/100\n",
      "2/3 [===================>..........] - ETA: 40s - loss: 135.3330 - distance_loss: 135.3330Epoch 00021: distance_loss did not improve\n",
      "3/3 [==============================] - 192s - loss: 131.8684 - distance_loss: 131.8684 - val_loss: 135.4658 - val_distance_loss: 135.4658\n",
      "Epoch 23/100\n",
      "2/3 [===================>..........] - ETA: 61s - loss: 110.9658 - distance_loss: 110.9658Epoch 00022: distance_loss did not improve\n",
      "3/3 [==============================] - 375s - loss: 122.9185 - distance_loss: 122.9185 - val_loss: 122.6870 - val_distance_loss: 122.6870\n",
      "Epoch 24/100\n",
      "2/3 [===================>..........] - ETA: 75s - loss: 104.9785 - distance_loss: 104.9785Epoch 00023: distance_loss improved from 114.26816 to 110.71760, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.23-110.72.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 410s - loss: 110.7176 - distance_loss: 110.7176 - val_loss: 53.0676 - val_distance_loss: 53.0676\n",
      "Epoch 25/100\n",
      "2/3 [===================>..........] - ETA: 104s - loss: 127.3273 - distance_loss: 127.3273Epoch 00024: distance_loss did not improve\n",
      "3/3 [==============================] - 497s - loss: 122.8553 - distance_loss: 122.8553 - val_loss: 155.9709 - val_distance_loss: 155.9710\n",
      "Epoch 26/100\n",
      "2/3 [===================>..........] - ETA: 76s - loss: 106.5156 - distance_loss: 106.5156Epoch 00025: distance_loss did not improve\n",
      "3/3 [==============================] - 341s - loss: 115.4463 - distance_loss: 115.4463 - val_loss: 61.0053 - val_distance_loss: 61.0053\n",
      "Epoch 27/100\n",
      "2/3 [===================>..........] - ETA: 47s - loss: 112.0647 - distance_loss: 112.0648 Epoch 00026: distance_loss improved from 110.71760 to 108.08206, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.26-108.08.hdf5\n",
      "3/3 [==============================] - 305s - loss: 108.0820 - distance_loss: 108.0821 - val_loss: 65.0502 - val_distance_loss: 65.0502\n",
      "Epoch 28/100\n",
      "2/3 [===================>..........] - ETA: 47s - loss: 123.4914 - distance_loss: 123.4914Epoch 00027: distance_loss did not improve\n",
      "3/3 [==============================] - 425s - loss: 116.5501 - distance_loss: 116.5501 - val_loss: 155.0012 - val_distance_loss: 155.0012\n",
      "Epoch 29/100\n",
      "2/3 [===================>..........] - ETA: 56s - loss: 127.4135 - distance_loss: 127.4135Epoch 00028: distance_loss did not improve\n",
      "3/3 [==============================] - 150s - loss: 124.7628 - distance_loss: 124.7628 - val_loss: 166.0522 - val_distance_loss: 166.0523\n",
      "Epoch 30/100\n",
      "2/3 [===================>..........] - ETA: 60s - loss: 116.6052 - distance_loss: 116.6052Epoch 00029: distance_loss did not improve\n",
      "3/3 [==============================] - 251s - loss: 110.8716 - distance_loss: 110.8716 - val_loss: 247.6067 - val_distance_loss: 247.6066\n",
      "Epoch 31/100\n",
      "2/3 [===================>..........] - ETA: 127s - loss: 118.8872 - distance_loss: 118.8873Epoch 00030: distance_loss did not improve\n",
      "3/3 [==============================] - 360s - loss: 133.4256 - distance_loss: 133.4256 - val_loss: 151.3705 - val_distance_loss: 151.3705\n",
      "Epoch 32/100\n",
      "2/3 [===================>..........] - ETA: 105s - loss: 124.4700 - distance_loss: 124.4699Epoch 00031: distance_loss did not improve\n",
      "3/3 [==============================] - 498s - loss: 135.3680 - distance_loss: 135.3680 - val_loss: 268.2695 - val_distance_loss: 268.2695\n",
      "Epoch 33/100\n",
      "2/3 [===================>..........] - ETA: 65s - loss: 98.3038 - distance_loss: 98.3038Epoch 00032: distance_loss improved from 108.08206 to 105.64659, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.32-105.65.hdf5\n",
      "3/3 [==============================] - 374s - loss: 105.6466 - distance_loss: 105.6466 - val_loss: 126.9526 - val_distance_loss: 126.9526\n",
      "Epoch 34/100\n",
      "2/3 [===================>..........] - ETA: 85s - loss: 105.5273 - distance_loss: 105.5273Epoch 00033: distance_loss did not improve\n",
      "3/3 [==============================] - 386s - loss: 113.4995 - distance_loss: 113.4995 - val_loss: 192.2747 - val_distance_loss: 192.2747\n",
      "Epoch 35/100\n",
      "2/3 [===================>..........] - ETA: 132s - loss: 103.8325 - distance_loss: 103.8325Epoch 00034: distance_loss improved from 105.64659 to 96.91923, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug/unet_paper_0.005_custom_aug_.34-96.92.hdf5\n",
      "3/3 [==============================] - 542s - loss: 96.9192 - distance_loss: 96.9192 - val_loss: 139.6434 - val_distance_loss: 139.6434\n",
      "Epoch 36/100\n",
      "2/3 [===================>..........] - ETA: 12s - loss: 104.0700 - distance_loss: 104.0700Epoch 00035: distance_loss did not improve\n",
      "3/3 [==============================] - 114s - loss: 102.4307 - distance_loss: 102.4307 - val_loss: 135.3019 - val_distance_loss: 135.3019\n",
      "Epoch 37/100\n",
      "2/3 [===================>..........] - ETA: 24s - loss: 111.3303 - distance_loss: 111.3304Epoch 00036: distance_loss did not improve\n",
      "3/3 [==============================] - 171s - loss: 104.2467 - distance_loss: 104.2467 - val_loss: 262.4706 - val_distance_loss: 262.4706\n",
      "Epoch 38/100\n",
      "2/3 [===================>..........] - ETA: 105s - loss: 155.9739 - distance_loss: 155.9739Epoch 00037: distance_loss did not improve\n",
      "3/3 [==============================] - 356s - loss: 141.1445 - distance_loss: 141.1445 - val_loss: 327.5416 - val_distance_loss: 327.5416\n",
      "Epoch 39/100\n",
      "2/3 [===================>..........] - ETA: 125s - loss: 126.8870 - distance_loss: 126.8871Epoch 00038: distance_loss did not improve\n",
      "3/3 [==============================] - 405s - loss: 130.5885 - distance_loss: 130.5885 - val_loss: 150.6278 - val_distance_loss: 150.6278\n",
      "Epoch 40/100\n",
      "2/3 [===================>..........] - ETA: 111s - loss: 110.6426 - distance_loss: 110.6426Epoch 00039: distance_loss did not improve\n",
      "3/3 [==============================] - 412s - loss: 108.3946 - distance_loss: 108.3946 - val_loss: 155.6467 - val_distance_loss: 155.6467\n",
      "Epoch 41/100\n",
      "2/3 [===================>..........] - ETA: 96s - loss: 108.0330 - distance_loss: 108.0330 Epoch 00040: distance_loss did not improve\n",
      "3/3 [==============================] - 507s - loss: 122.9868 - distance_loss: 122.9868 - val_loss: 112.6679 - val_distance_loss: 112.6679\n",
      "Epoch 42/100\n",
      "2/3 [===================>..........] - ETA: 43s - loss: 102.0015 - distance_loss: 102.0015Epoch 00041: distance_loss did not improve\n",
      "3/3 [==============================] - 286s - loss: 99.9071 - distance_loss: 99.9071 - val_loss: 280.5417 - val_distance_loss: 280.5417\n",
      "Epoch 43/100\n",
      "2/3 [===================>..........] - ETA: 52s - loss: 112.3572 - distance_loss: 112.3572Epoch 00042: distance_loss did not improve\n",
      "3/3 [==============================] - 270s - loss: 121.5823 - distance_loss: 121.5823 - val_loss: 198.2873 - val_distance_loss: 198.2872\n",
      "Epoch 44/100\n",
      "2/3 [===================>..........] - ETA: 57s - loss: 85.7942 - distance_loss: 85.7942Epoch 00043: distance_loss did not improve\n",
      "3/3 [==============================] - 249s - loss: 98.5590 - distance_loss: 98.5590 - val_loss: 50.4524 - val_distance_loss: 50.4524\n",
      "Epoch 45/100\n",
      "2/3 [===================>..........] - ETA: 51s - loss: 101.5442 - distance_loss: 101.5442Epoch 00044: distance_loss did not improve\n",
      "3/3 [==============================] - 292s - loss: 110.2859 - distance_loss: 110.2859 - val_loss: 47.1560 - val_distance_loss: 47.1560\n",
      "Epoch 46/100\n",
      "2/3 [===================>..........] - ETA: 93s - loss: 124.0459 - distance_loss: 124.0459 Epoch 00045: distance_loss did not improve\n",
      "3/3 [==============================] - 353s - loss: 117.5153 - distance_loss: 117.5153 - val_loss: 104.4965 - val_distance_loss: 104.4965\n",
      "Epoch 47/100\n",
      "2/3 [===================>..........] - ETA: 31s - loss: 109.2493 - distance_loss: 109.2493Epoch 00046: distance_loss did not improve\n",
      "3/3 [==============================] - 304s - loss: 105.1013 - distance_loss: 105.1013 - val_loss: 315.7985 - val_distance_loss: 315.7986\n",
      "Epoch 48/100\n",
      "2/3 [===================>..........] - ETA: 33s - loss: 99.5114 - distance_loss: 99.5114  Epoch 00047: distance_loss did not improve\n",
      "3/3 [==============================] - 213s - loss: 121.8319 - distance_loss: 121.8319 - val_loss: 157.8789 - val_distance_loss: 157.8789\n",
      "Epoch 49/100\n",
      "2/3 [===================>..........] - ETA: 80s - loss: 101.3736 - distance_loss: 101.3736 Epoch 00048: distance_loss did not improve\n",
      "3/3 [==============================] - 243s - loss: 124.1634 - distance_loss: 124.1634 - val_loss: 358.0864 - val_distance_loss: 358.0865\n",
      "Epoch 50/100\n",
      "2/3 [===================>..........] - ETA: 22s - loss: 93.2176 - distance_loss: 93.2176Epoch 00049: distance_loss did not improve\n",
      "3/3 [==============================] - 126s - loss: 106.3842 - distance_loss: 106.3842 - val_loss: 157.2379 - val_distance_loss: 157.2379\n",
      "Epoch 51/100\n",
      "2/3 [===================>..........] - ETA: 73s - loss: 123.2061 - distance_loss: 123.2061 Epoch 00050: distance_loss did not improve\n",
      "3/3 [==============================] - 336s - loss: 122.6562 - distance_loss: 122.6562 - val_loss: 160.4184 - val_distance_loss: 160.4184\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import keras\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dropout, Flatten, Reshape, Input\n",
    "from keras.layers.core import Activation, Dense, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "learning_rate=.005\n",
    "epochs=100\n",
    "batch_size=4\n",
    "data_loc='/home/rbbidart/project/rbbidart/cancer_hist/full_slides2'\n",
    "mask_loc='/home/rbbidart/project/rbbidart/cancer_hist/im_dist_labels'\n",
    "out_loc='/home/rbbidart/cancer_hist_out/unet_dist/unet_paper_custom_aug'\n",
    "\n",
    "\n",
    "# Locations\n",
    "train_loc = os.path.join(str(data_loc),'train', str(0))\n",
    "train_mask_loc = os.path.join(str(mask_loc),'train', str(0))\n",
    "\n",
    "valid_loc = os.path.join(str(data_loc),'valid', str(0))\n",
    "valid_mask_loc = os.path.join(str(mask_loc),'valid', str(0))\n",
    "\n",
    "num_train = 12 #len(glob.glob(os.path.join(train_loc, '*')))/2-2\n",
    "num_valid = 4# len(glob.glob(os.path.join(valid_loc, '*')))/2-2\n",
    "print(valid_loc)\n",
    "print('num_train', num_train)\n",
    "print('num_valid', num_valid)\n",
    "\n",
    "# Params for all models\n",
    "batch_size=int(batch_size)   # make this divisible by len(x_data)\n",
    "steps_per_epoch = np.floor(num_train/batch_size) # num of batches from generator at each epoch. (make it full train set)\n",
    "validation_steps = np.floor(num_valid/batch_size)# size of validation dataset divided by batch size\n",
    "print('validation_steps', validation_steps)\n",
    "\n",
    "model = unet_paper(learning_rate=learning_rate)\n",
    "name = 'unet_paper'+'_'+str(learning_rate)+'_'+'custom_aug'\n",
    "out_file=os.path.join(str(out_loc), name)\n",
    "\n",
    "# need a batch generator to augment the labels same as the train images\n",
    "valid_generator = data_gen_combined(valid_loc, valid_mask_loc, batch_size, seed=101)\n",
    "train_generator = data_gen_aug_combined(train_loc, train_mask_loc, batch_size, square_rot_p=.3,  seed=101)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='distance_loss', patience=15, verbose=0),\n",
    "        ModelCheckpoint(filepath=os.path.join(out_loc, name + '_.{epoch:02d}-{distance_loss:.2f}.hdf5'), \n",
    "        verbose=1, monitor='distance_loss', save_best_only=True)]\n",
    "\n",
    "hist = model.fit_generator(train_generator,\n",
    "                                  validation_data=valid_generator,\n",
    "                                  steps_per_epoch=steps_per_epoch, \n",
    "                                  epochs=epochs,\n",
    "                                  validation_steps=validation_steps,\n",
    "                                  callbacks=callbacks)\n",
    "pickle.dump(hist.history, open(out_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rbbidart/project/rbbidart/cancer_hist/full_slides2/valid/0\n",
      "num_train 87.0\n",
      "num_valid 20.0\n",
      "validation_steps 2.0\n",
      "Epoch 1/100\n",
      " 9/10 [==========================>...] - ETA: 164s - loss: 326.4845 - distance_loss: 326.4845Epoch 00000: distance_loss improved from inf to 367.98170, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_standard_custom_aug/unet_standard_0.005_custom_aug_.00-367.98.hdf5\n",
      "10/10 [==============================] - 2063s - loss: 367.9816 - distance_loss: 367.9817 - val_loss: 125.1759 - val_distance_loss: 125.1759\n",
      "Epoch 2/100\n",
      " 9/10 [==========================>...] - ETA: 150s - loss: 178.9617 - distance_loss: 178.9616Epoch 00001: distance_loss improved from 367.98170 to 188.58488, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_standard_custom_aug/unet_standard_0.005_custom_aug_.01-188.58.hdf5\n",
      "10/10 [==============================] - 1914s - loss: 188.5849 - distance_loss: 188.5849 - val_loss: 129.1050 - val_distance_loss: 129.1050\n",
      "Epoch 3/100\n",
      " 9/10 [==========================>...] - ETA: 137s - loss: 120.1366 - distance_loss: 120.1367Epoch 00002: distance_loss improved from 188.58488 to 120.62914, saving model to /home/rbbidart/cancer_hist_out/unet_dist/unet_standard_custom_aug/unet_standard_0.005_custom_aug_.02-120.63.hdf5\n",
      "10/10 [==============================] - 1944s - loss: 120.6291 - distance_loss: 120.6291 - val_loss: 130.6416 - val_distance_loss: 130.6416\n",
      "Epoch 4/100\n",
      " 5/10 [==============>...............] - ETA: 58s - loss: 119.7285 - distance_loss: 119.7284"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import keras\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dropout, Flatten, Reshape, Input\n",
    "from keras.layers.core import Activation, Dense, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "learning_rate=.005\n",
    "epochs=100\n",
    "batch_size=8\n",
    "data_loc='/home/rbbidart/project/rbbidart/cancer_hist/full_slides2'\n",
    "mask_loc='/home/rbbidart/project/rbbidart/cancer_hist/im_dist_labels'\n",
    "out_loc='/home/rbbidart/cancer_hist_out/unet_dist/unet_standard_custom_aug'\n",
    "\n",
    "\n",
    "# Locations\n",
    "train_loc = os.path.join(str(data_loc),'train', str(0))\n",
    "train_mask_loc = os.path.join(str(mask_loc),'train', str(0))\n",
    "\n",
    "valid_loc = os.path.join(str(data_loc),'valid', str(0))\n",
    "valid_mask_loc = os.path.join(str(mask_loc),'valid', str(0))\n",
    "\n",
    "num_train = len(glob.glob(os.path.join(train_loc, '*')))/2-2\n",
    "num_valid = len(glob.glob(os.path.join(valid_loc, '*')))/2-2\n",
    "print(valid_loc)\n",
    "print('num_train', num_train)\n",
    "print('num_valid', num_valid)\n",
    "\n",
    "# Params for all models\n",
    "batch_size=int(batch_size)   # make this divisible by len(x_data)\n",
    "steps_per_epoch = np.floor(num_train/batch_size) # num of batches from generator at each epoch. (make it full train set)\n",
    "validation_steps = np.floor(num_valid/batch_size)# size of validation dataset divided by batch size\n",
    "print('validation_steps', validation_steps)\n",
    "\n",
    "model = unet_standard(learning_rate=learning_rate)\n",
    "name = 'unet_standard'+'_'+str(learning_rate)+'_'+'custom_aug'\n",
    "out_file=os.path.join(str(out_loc), name)\n",
    "\n",
    "# need a batch generator to augment the labels same as the train images\n",
    "valid_generator = data_gen_combined(valid_loc, valid_mask_loc, batch_size, seed=101)\n",
    "train_generator = data_gen_aug_combined(train_loc, train_mask_loc, batch_size, square_rot_p=.3,  seed=101)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='distance_loss', patience=15, verbose=0),\n",
    "        ModelCheckpoint(filepath=os.path.join(out_loc, name + '_.{epoch:02d}-{distance_loss:.2f}.hdf5'), \n",
    "        verbose=1, monitor='distance_loss', save_best_only=True)]\n",
    "\n",
    "hist = model.fit_generator(train_generator,\n",
    "                                  validation_data=valid_generator,\n",
    "                                  steps_per_epoch=steps_per_epoch, \n",
    "                                  epochs=epochs,\n",
    "                                  validation_steps=validation_steps,\n",
    "                                  callbacks=callbacks)\n",
    "pickle.dump(hist.history, open(out_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "data_loc = '/home/rbbidart/project/rbbidart/cancer_hist/full_slides2'\n",
    "out_loc = '/home/rbbidart/project/rbbidart/cancer_hist/full_slides2_k'\n",
    "\n",
    "all_images=glob.glob(data_loc + '/**/*.tif', recursive=True)\n",
    "for image_file in all_images:\n",
    "    name = image_file.rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "    new_loc = image_file.rsplit('/', 1)[0].replace('full_slides2', 'full_slides2_k')\n",
    "    if not os.path.exists(new_loc):\n",
    "        os.makedirs(new_loc)\n",
    "    new_name = name+'.jpg'\n",
    "    im = Image.open(image_file)\n",
    "    im.save(os.path.join(new_loc, new_name))\n",
    "    \n",
    "data_loc = '/home/rbbidart/project/rbbidart/cancer_hist/im_dist_labels'\n",
    "out_loc = '/home/rbbidart/project/rbbidart/cancer_hist/im_dist_labels_k'\n",
    "all_images=glob.glob(data_loc + '/**/*.tif', recursive=True)\n",
    "for image_file in all_images:\n",
    "    name = image_file.rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "    new_loc = image_file.rsplit('/', 1)[0].replace('im_dist_labels', 'im_dist_labels_k')\n",
    "    if not os.path.exists(new_loc):\n",
    "        os.makedirs(new_loc)\n",
    "    new_name = name+'.jpg'\n",
    "    im = Image.open(image_file)\n",
    "    im.save(os.path.join(new_loc, new_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def get_generator(train_folder, train_mask_folder, valid_folder, valid_mask_folder,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    rotation_range=10,\n",
    "                    zoom_range=0.2,\n",
    "                    classes=['keras'],\n",
    "                    fill_mode=\"constant\"):\n",
    "    batch_size = 4\n",
    "    seed = 42\n",
    "    # Example taken from https://keras.io/preprocessing/image/\n",
    "    # We create two instances with the same arguments\n",
    "    data_gen_args_train = dict(\n",
    "                        width_shift_range=width_shift_range,\n",
    "                        height_shift_range=height_shift_range,\n",
    "                        horizontal_flip=horizontal_flip,\n",
    "                        rotation_range=rotation_range,\n",
    "                        zoom_range=zoom_range,\n",
    "                        fill_mode=fill_mode, \n",
    "                        cval=0       \n",
    "                        )\n",
    "    data_gen_args_masks = dict(\n",
    "                        width_shift_range=width_shift_range,\n",
    "                        height_shift_range=height_shift_range,\n",
    "                        horizontal_flip=horizontal_flip,\n",
    "                        rotation_range=rotation_range,\n",
    "                        zoom_range=zoom_range,\n",
    "                        fill_mode=fill_mode,\n",
    "                        cval=0    \n",
    "                        )\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args_train)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args_masks)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_folder,\n",
    "        batch_size = batch_size,\n",
    "        target_size = (640, 800),\n",
    "        class_mode=None,\n",
    "        color_mode='rgb',\n",
    "        classes=classes,\n",
    "        seed=seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_mask_folder,\n",
    "        batch_size = batch_size,    \n",
    "        target_size = (640, 800),    \n",
    "        class_mode=None,\n",
    "        color_mode='grayscale',\n",
    "        classes=classes,\n",
    "        seed=seed)\n",
    "    valid_image_generator = image_datagen.flow_from_directory(\n",
    "        valid_folder,\n",
    "        batch_size = batch_size,\n",
    "        target_size = (640, 800),\n",
    "        class_mode=None,\n",
    "        color_mode='rgb',\n",
    "        classes=classes,\n",
    "        seed=seed)\n",
    "    valid_mask_generator = mask_datagen.flow_from_directory(\n",
    "        valid_mask_folder,\n",
    "        batch_size = batch_size,    \n",
    "        target_size = (640, 800),    \n",
    "        class_mode=None,\n",
    "        color_mode='grayscale',\n",
    "        classes=classes,\n",
    "        seed=seed)\n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    valid_generator = zip(valid_image_generator, valid_mask_generator)   \n",
    "    return train_generator,valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_loc='/home/rbbidart/cancer_hist_out/unet_dist/unet_standard_keras_aug'\n",
    "learning_rate=.005\n",
    "epochs=100\n",
    "batch_size=8\n",
    "data_loc='/home/rbbidart/project/rbbidart/cancer_hist/full_slides2_k'\n",
    "mask_loc='/home/rbbidart/project/rbbidart/cancer_hist/im_dist_labels_k'\n",
    "out_loc='/home/rbbidart/cancer_hist_out/unet_dist/unet_standard_custom_aug'\n",
    "\n",
    "\n",
    "train_loc = os.path.join(str(data_loc),'train')\n",
    "train_loc_mask = os.path.join(str(mask_loc),'train')\n",
    "\n",
    "valid_loc = os.path.join(str(data_loc),'valid')\n",
    "valid_loc_mask = os.path.join(str(mask_loc),'valid')\n",
    "\n",
    "\n",
    "train_generator,valid_generator=get_generator(train_loc, train_loc_mask, \n",
    "                                              valid_loc, valid_loc_mask,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    rotation_range=180,\n",
    "                    zoom_range=0.3,\n",
    "                    classes=['keras'],\n",
    "                    fill_mode=\"constant\")\n",
    "\n",
    "\n",
    "num_train = len(glob.glob(train_loc + '/**/*.png', recursive=True))\n",
    "num_valid = len(glob.glob(valid_loc + '/**/*.png', recursive=True))\n",
    "print(valid_loc)\n",
    "print('num_train', num_train)\n",
    "print('num_valid', num_valid)\n",
    "\n",
    "# Params for all models\n",
    "batch_size=int(batch_size)   # make this divisible by len(x_data)\n",
    "steps_per_epoch = np.floor(num_train/batch_size) # num of batches from generator at each epoch. (make it full train set)\n",
    "validation_steps = np.floor(num_valid/batch_size)# size of validation dataset divided by batch size\n",
    "print('validation_steps', validation_steps)\n",
    "\n",
    "model = unet_standard(learning_rate=learning_rate)\n",
    "name = 'unet_standard'+'_'+str(learning_rate)+'_'+'keras_aug'\n",
    "out_file=os.path.join(str(out_loc), name)\n",
    "\n",
    "# need a batch generator to augment the labels same as the train images\n",
    "valid_generator = data_gen_combined(valid_loc, valid_mask_loc, batch_size, seed=101)\n",
    "train_generator = data_gen_aug_combined(train_loc, train_mask_loc, batch_size, square_rot_p=.3,  seed=101)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='distance_loss', patience=15, verbose=0),\n",
    "        ModelCheckpoint(filepath=os.path.join(out_loc, name + '_.{epoch:02d}-{distance_loss:.2f}.hdf5'), \n",
    "        verbose=1, monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "hist = model.fit_generator(train_generator,\n",
    "                                  validation_data=valid_generator,\n",
    "                                  steps_per_epoch=steps_per_epoch, \n",
    "                                  epochs=epochs,\n",
    "                                  validation_steps=validation_steps,\n",
    "                                  callbacks=callbacks)\n",
    "pickle.dump(hist.history, open(out_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
